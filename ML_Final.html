<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Gavin Kim" />

<meta name="date" content="2016-12-15" />

<title>Practical Machine Learning - Prediction Model for Weight Lifting Exercise Dataset</title>

<script src="ML_Final_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="ML_Final_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="ML_Final_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="ML_Final_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="ML_Final_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="ML_Final_files/navigation-1.1/tabsets.js"></script>
<link href="ML_Final_files/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="ML_Final_files/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Practical Machine Learning - Prediction Model for Weight Lifting Exercise Dataset</h1>
<h4 class="author"><em>Gavin Kim</em></h4>
<h4 class="date"><em>2016-12-15</em></h4>

</div>


<div id="executive-summary" class="section level2">
<h2>Executive Summary</h2>
<p>This is final report of Coursera Practical Machine Learning Course. Goal of this document is to build a prediction model of Weight Lifting Exercise. The dataset contains variables from sensor worn by users and the classes that type of exercise. For match this goal, I used ensemble model of Generalized Boosted Regression Model, Random Forest, Conditional Inference Tree. And for reducing dimension to increase training speed, PCA is used. But the ensenble got no more performance of model than Random Forest. After all this approach, I could get 0.98% of accuracy.</p>
</div>
<div id="loading-data" class="section level2">
<h2>Loading data</h2>
<div id="load-libraries" class="section level4">
<h4>Load libraries</h4>
<pre class="r"><code>library(caret)
library(randomForest)
library(corrplot)
library(MASS)

# For increasing learning speed, use multi-core.
library(doMC)
registerDoMC(cores = 8)</code></pre>
</div>
<div id="download-and-load-trainingtest-data" class="section level4">
<h4>Download and Load training/test data</h4>
<pre class="r"><code># Check existance of data directory and create it if not exist.
dataDir &lt;- &quot;data&quot;
if(!dir.exists(dataDir))
  dir.create(dataDir)

# Download training and test file 
Url.train &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;
Url.test &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;
data.trainPath &lt;- paste0(dataDir, &quot;/&quot;, basename(Url.train))
data.testPath &lt;- paste0(dataDir, &quot;/&quot;, basename(Url.test))

if(!file.exists(data.trainPath))
  download.file(Url.train, data.trainPath, mode = &quot;w&quot;)
if(!file.exists(data.testPath))
  download.file(Url.test, data.testPath, mode = &quot;w&quot;)

data.train &lt;- read.csv(data.trainPath)
data.test &lt;- read.csv(data.testPath)</code></pre>
</div>
</div>
<div id="preprocessing-data" class="section level2">
<h2>Preprocessing data</h2>
<p>After loaing data, we need to remove the columns what we don’t use. There are columns that have only NA values and nearly zero variance. It’s better that we could replace these columns from data and there’s columns not related to predict weight lifting exercise like observation number or user name or timestamps. It also has to be removed.</p>
<pre class="r"><code># Remove columns having NA values
cs &lt;- colSums(is.na(data.train))
data.train &lt;- data.train[,(cs == 0)]
data.test &lt;- data.test[,(cs == 0)]

# Remove near zero columns
nzvColumns &lt;- nearZeroVar(data.train,saveMetrics=TRUE)
data.train &lt;- data.train[,!nzvColumns$nzv]
data.test &lt;- data.test[,!nzvColumns$nzv]

# Remove unnecessary columns (Username, timesstamps)
data.train &lt;- data.train[,-c(1:6)]
data.test &lt;- data.test[,-c(1:6)]</code></pre>
<p>With checking correlation plot, we can get there’s no strong relation between variables to disadventage building model.</p>
<pre class="r"><code>corrplot(cor(data.train[,-53]), method=&quot;pie&quot;, type=&quot;upper&quot;)</code></pre>
<p><img src="ML_Final_files/figure-html/corr-1.png" /><!-- --></p>
<div id="split-data-to-training-and-validation-set" class="section level4">
<h4>Split data to training and validation set</h4>
<p>After cleaning data, I found the test data set has only 20 observasions. It’s too small to check the accuracy of model. Hence, I decided to split training data with training and validation. The validation data is for test model’s accuracy and I’ll use training data for building model with K-fold cross validation.</p>
<pre class="r"><code># split data for training/validation
inTrain &lt;- createDataPartition(y=data.train$classe, p=0.7, list=FALSE)
data.trainPart &lt;- data.train[inTrain,]
data.validPart &lt;- data.train[-inTrain,]</code></pre>
</div>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<p>I made plan to use ensemble model with stacking gbm, rf, ctree And PCA can reduce the size of variables for training performance.</p>
<pre class="r"><code># 53th column is &quot;classe&quot; what will used for outcome label.
pcaModel &lt;- preProcess(data.trainPart[,-53], method=c(&quot;pca&quot;, &quot;center&quot;, &quot;scale&quot;), thresh = 0.95)
data.trainPca &lt;- predict(pcaModel, data.trainPart[,-53])
data.trainPca &lt;- data.frame(data.trainPca, classe = data.trainPart[,53])
data.validPca &lt;- predict(pcaModel, data.validPart[,-53])
data.validPca &lt;- data.frame(data.validPca, classe = data.validPart[,53])
print(pcaModel)</code></pre>
<pre><code>## Created from 13737 samples and 52 variables
## 
## Pre-processing:
##   - centered (52)
##   - ignored (0)
##   - principal component signal extraction (52)
##   - scaled (52)
## 
## PCA needed 25 components to capture 95 percent of the variance</code></pre>
<p>As you see above 25 components has captured with 95 percent of the variance.</p>
<pre class="r"><code>set.seed(1234)
fit_gbm &lt;- train(classe ~ ., data = data.trainPca, method = &quot;gbm&quot;)</code></pre>
<pre><code>## Loading required package: gbm</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## 
## Attaching package: &#39;survival&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:caret&#39;:
## 
##     cluster</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## Loaded gbm 2.1.1</code></pre>
<pre><code>## Loading required package: plyr</code></pre>
<pre><code>## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        1.6094             nan     0.1000    0.1168
##      2        1.5352             nan     0.1000    0.0799
##      3        1.4829             nan     0.1000    0.0776
##      4        1.4347             nan     0.1000    0.0628
##      5        1.3963             nan     0.1000    0.0506
##      6        1.3633             nan     0.1000    0.0455
##      7        1.3337             nan     0.1000    0.0421
##      8        1.3073             nan     0.1000    0.0368
##      9        1.2828             nan     0.1000    0.0314
##     10        1.2614             nan     0.1000    0.0335
##     20        1.1007             nan     0.1000    0.0166
##     40        0.9251             nan     0.1000    0.0082
##     60        0.8188             nan     0.1000    0.0064
##     80        0.7412             nan     0.1000    0.0056
##    100        0.6761             nan     0.1000    0.0026
##    120        0.6233             nan     0.1000    0.0020
##    140        0.5763             nan     0.1000    0.0021
##    150        0.5570             nan     0.1000    0.0016</code></pre>
<pre class="r"><code>pred_gbm &lt;- predict(fit_gbm, newdata = data.validPca)
confusionMatrix(pred_gbm,data.validPca$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1509  140   57   24   31
##          B   38  863   92   23   79
##          C   55   81  826  112   60
##          D   65   18   26  779   42
##          E    7   37   25   26  870
## 
## Overall Statistics
##                                           
##                Accuracy : 0.8236          
##                  95% CI : (0.8136, 0.8333)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.7765          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9014   0.7577   0.8051   0.8081   0.8041
## Specificity            0.9402   0.9511   0.9366   0.9693   0.9802
## Pos Pred Value         0.8569   0.7881   0.7284   0.8376   0.9016
## Neg Pred Value         0.9600   0.9424   0.9579   0.9627   0.9569
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2564   0.1466   0.1404   0.1324   0.1478
## Detection Prevalence   0.2992   0.1861   0.1927   0.1580   0.1640
## Balanced Accuracy      0.9208   0.8544   0.8708   0.8887   0.8921</code></pre>
<pre class="r"><code>set.seed(1234)
fit_rf &lt;- train(classe ~ ., data = data.trainPca, method = &quot;rf&quot;)
pred_rf &lt;- predict(fit_rf, newdata = data.validPca)
confusionMatrix(pred_rf,data.validPca$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1668   16    1    1    0
##          B    0 1104   21    2    3
##          C    3   14  997   36    8
##          D    3    0    3  922    3
##          E    0    5    4    3 1068
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9786          
##                  95% CI : (0.9746, 0.9821)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9729          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9964   0.9693   0.9717   0.9564   0.9871
## Specificity            0.9957   0.9945   0.9874   0.9982   0.9975
## Pos Pred Value         0.9893   0.9770   0.9423   0.9903   0.9889
## Neg Pred Value         0.9986   0.9926   0.9940   0.9915   0.9971
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2834   0.1876   0.1694   0.1567   0.1815
## Detection Prevalence   0.2865   0.1920   0.1798   0.1582   0.1835
## Balanced Accuracy      0.9961   0.9819   0.9796   0.9773   0.9923</code></pre>
<pre class="r"><code>set.seed(1234)
fit_ctree &lt;- train(classe ~ ., data = data.trainPca, method = &quot;ctree&quot;)</code></pre>
<pre><code>## Loading required package: party</code></pre>
<pre><code>## Loading required package: grid</code></pre>
<pre><code>## Loading required package: mvtnorm</code></pre>
<pre><code>## Loading required package: modeltools</code></pre>
<pre><code>## Loading required package: stats4</code></pre>
<pre><code>## 
## Attaching package: &#39;modeltools&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:plyr&#39;:
## 
##     empty</code></pre>
<pre><code>## Loading required package: strucchange</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre><code>## Loading required package: sandwich</code></pre>
<pre class="r"><code>pred_ctree &lt;- predict(fit_ctree, newdata = data.validPca)
confusionMatrix(pred_ctree,data.validPca$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1447  135   60   55   33
##          B   82  797  103   54   84
##          C   62   80  753   83   51
##          D   63   53   60  724   72
##          E   20   74   50   48  842
## 
## Overall Statistics
##                                          
##                Accuracy : 0.7754         
##                  95% CI : (0.7645, 0.786)
##     No Information Rate : 0.2845         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.7155         
##  Mcnemar&#39;s Test P-Value : 0.001367       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.8644   0.6997   0.7339   0.7510   0.7782
## Specificity            0.9328   0.9319   0.9432   0.9496   0.9600
## Pos Pred Value         0.8364   0.7116   0.7318   0.7449   0.8143
## Neg Pred Value         0.9454   0.9282   0.9438   0.9512   0.9505
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2459   0.1354   0.1280   0.1230   0.1431
## Detection Prevalence   0.2940   0.1903   0.1749   0.1652   0.1757
## Balanced Accuracy      0.8986   0.8158   0.8386   0.8503   0.8691</code></pre>
<pre class="r"><code>predDF &lt;- data.frame(pred_gbm, pred_rf, pred_ctree, classe = data.validPca$classe)
fit_stacked &lt;- train(classe ~ ., data = predDF, method=&quot;rf&quot;)
pred_stacked &lt;- predict(fit_stacked, newdata = predDF)
confusionMatrix(pred_stacked,data.validPca$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1668   16    1    1    0
##          B    0 1104   21    2    3
##          C    3   14  997   36    8
##          D    3    0    3  922    3
##          E    0    5    4    3 1068
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9786          
##                  95% CI : (0.9746, 0.9821)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9729          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9964   0.9693   0.9717   0.9564   0.9871
## Specificity            0.9957   0.9945   0.9874   0.9982   0.9975
## Pos Pred Value         0.9893   0.9770   0.9423   0.9903   0.9889
## Neg Pred Value         0.9986   0.9926   0.9940   0.9915   0.9971
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2834   0.1876   0.1694   0.1567   0.1815
## Detection Prevalence   0.2865   0.1920   0.1798   0.1582   0.1835
## Balanced Accuracy      0.9961   0.9819   0.9796   0.9773   0.9923</code></pre>
<p>The model what has best performance is Random forest as you see. And the ensemble model of gbm, rf, ctree gets no benefit of accuracy.</p>
</div>
<div id="testing" class="section level2">
<h2>Testing</h2>
<p>We need answer to predict the 20 test set with this model. Here is the results.</p>
<pre class="r"><code># Apply PCA
data.testPCA &lt;- predict(pcaModel, data.test[,-53])

# Create predicted variales
pred_gbm_test &lt;- predict(fit_gbm, newdata = data.testPCA)
pred_rf_test &lt;- predict(fit_rf, newdata = data.testPCA)
pred_ctree_test &lt;- predict(fit_ctree, newdata = data.testPCA)

predDF_test &lt;- data.frame(pred_gbm = pred_gbm_test, pred_rf = pred_rf_test, pred_ctree = pred_ctree_test)
pred_stacked_test &lt;- predict(fit_stacked, newdata = predDF_test)
print(t(data.frame(pred_stacked_test)))</code></pre>
<pre><code>##                   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
## pred_stacked_test &quot;B&quot;  &quot;A&quot;  &quot;B&quot;  &quot;A&quot;  &quot;A&quot;  &quot;E&quot;  &quot;D&quot;  &quot;B&quot;  &quot;A&quot;  &quot;A&quot;   &quot;B&quot;  
##                   [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
## pred_stacked_test &quot;C&quot;   &quot;B&quot;   &quot;A&quot;   &quot;E&quot;   &quot;E&quot;   &quot;A&quot;   &quot;B&quot;   &quot;B&quot;   &quot;B&quot;</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
